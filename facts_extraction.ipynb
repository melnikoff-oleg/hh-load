{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2020-12-22 17:24:00 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "| depparse  | syntagrus |\n",
      "| ner       | wikiner   |\n",
      "=========================\n",
      "\n",
      "2020-12-22 17:24:00 INFO: Use device: cpu\n",
      "2020-12-22 17:24:00 INFO: Loading: tokenize\n",
      "2020-12-22 17:24:00 INFO: Loading: pos\n",
      "2020-12-22 17:24:01 INFO: Loading: lemma\n",
      "2020-12-22 17:24:01 INFO: Loading: depparse\n",
      "2020-12-22 17:24:03 INFO: Loading: ner\n",
      "2020-12-22 17:24:05 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import stanza\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Downloading all needed models\n",
    "nltk.download(\"stopwords\")\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "nlp = stanza.Pipeline(lang='ru', processors='tokenize,pos,lemma,depparse,ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"Необходимы\",\n",
      "      \"lemma\": \"необходимый\",\n",
      "      \"upos\": \"ADJ\",\n",
      "      \"feats\": \"Degree=Pos|Number=Plur|Variant=Short\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"misc\": \"start_char=0|end_char=10\",\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"знания\",\n",
      "      \"lemma\": \"знание\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"feats\": \"Animacy=Inan|Case=Nom|Gender=Neut|Number=Plur\",\n",
      "      \"head\": 1,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"misc\": \"start_char=11|end_char=17\",\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"таких\",\n",
      "      \"lemma\": \"такой\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"feats\": \"Case=Gen|Number=Plur\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"det\",\n",
      "      \"misc\": \"start_char=18|end_char=23\",\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"технологий\",\n",
      "      \"lemma\": \"технология\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"feats\": \"Animacy=Inan|Case=Gen|Gender=Fem|Number=Plur\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"nmod\",\n",
      "      \"misc\": \"start_char=24|end_char=34\",\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"как\",\n",
      "      \"lemma\": \"как\",\n",
      "      \"upos\": \"SCONJ\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"mark\",\n",
      "      \"misc\": \"start_char=35|end_char=38\",\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \"Python\",\n",
      "      \"lemma\": \"Python\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"feats\": \"Foreign=Yes\",\n",
      "      \"head\": 4,\n",
      "      \"deprel\": \"flat:foreign\",\n",
      "      \"misc\": \"start_char=39|end_char=45\",\n",
      "      \"ner\": \"S-MISC\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"text\": \",\",\n",
      "      \"lemma\": \",\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"head\": 8,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"misc\": \"start_char=45|end_char=46\",\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"text\": \"Java\",\n",
      "      \"lemma\": \"Java\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"feats\": \"Foreign=Yes\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"flat:foreign\",\n",
      "      \"misc\": \"start_char=47|end_char=51\",\n",
      "      \"ner\": \"S-MISC\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 9,\n",
      "      \"text\": \",\",\n",
      "      \"lemma\": \",\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"head\": 8,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"misc\": \"start_char=51|end_char=52\",\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 10,\n",
      "      \"text\": \"PostgreSQL\",\n",
      "      \"lemma\": \"PostgreSQL\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"feats\": \"Foreign=Yes\",\n",
      "      \"head\": 1,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"misc\": \"start_char=53|end_char=63\",\n",
      "      \"ner\": \"S-MISC\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Необходимы знания таких технологий как Python, Java, PostgreSQL')\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_stopwords(doc):\n",
    "    r = doc.copy()\n",
    "    cur = 0\n",
    "    for ind, i in enumerate(doc):\n",
    "        if i['lemma'] in russian_stopwords or i['lemma'] in punctuation:\n",
    "            r.pop(ind - cur)\n",
    "            cur += 1\n",
    "    return r\n",
    "\n",
    "def doc_to_list(doc):\n",
    "    doc_json = json.loads(str(doc))\n",
    "    return doc_json[0]\n",
    "\n",
    "def lemmatized_sentence(doc):\n",
    "    arr = []\n",
    "    for i in doc:\n",
    "        arr.append(i['lemma'])\n",
    "    return ' '.join(arr)\n",
    "\n",
    "def process_doc(doc):\n",
    "    doc = doc_to_list(doc)\n",
    "    doc = del_stopwords(doc)\n",
    "    return doc\n",
    "\n",
    "def find_all_tech(doc):\n",
    "    ans = []\n",
    "    for i in doc:\n",
    "        if i['ner'][-4:] == 'MISC' and i['upos'] == 'PROPN':\n",
    "            ans.append(i['lemma'])\n",
    "    return list(set(ans))\n",
    "\n",
    "def get_lemms(doc):\n",
    "    ans = []\n",
    "    for i in doc:\n",
    "        ans.append(i['lemma'].lower())\n",
    "    return ans\n",
    "\n",
    "def check_if_flexible(lemms):\n",
    "    return ('гибкий' in lemms or 'удобный' in lemms or 'свободный' in lemms) and ('день' in lemms or 'график' in lemms or 'время' in lemms)\n",
    "\n",
    "def get_specializations(lemms):\n",
    "    keywords = [['docker', 'kubernetes', 'devops', 'развертывание', 'jenkins', 'ansible', 'ci', 'cd'], ['ml', 'dl', 'ds', 'machine', 'learning', 'data' ,'science', 'tensorflow', 'keras', 'pytorch', 'kaggle', 'comuter', 'vision'], ['test', 'testing', 'тест', 'тестирование', 'тестировщик']]\n",
    "    ans = [0, 0, 0, 0]\n",
    "    for i in lemms:\n",
    "        for ind, block in enumerate(keywords):\n",
    "            if i in block:\n",
    "                ans[ind] += 1\n",
    "    sm = ans[0] + ans[1] + ans[2]\n",
    "    if sm == 0:\n",
    "        ans[3] = 1\n",
    "    return ans\n",
    "\n",
    "def get_full_result(doc):\n",
    "    doc = process_doc(doc)\n",
    "    lemms = get_lemms(doc)\n",
    "    techs = find_all_tech(doc)\n",
    "    flexible = check_if_flexible(lemms)\n",
    "    specs = get_specializations(lemms)\n",
    "    return techs, flexible, specs\n",
    "\n",
    "def clear_string(s):\n",
    "    s = s.strip().replace('\\n', '').replace('\\r', '')\n",
    "    s = re.sub('[\"«»;?!,()]', '', s)\n",
    "    s = re.sub('[/—-]', ' ', s)\n",
    "    s = re.sub(r\"\\\\\", ' ', s)\n",
    "    s = re.sub('  ', ' ', s)\n",
    "    rus_alphavite = 'АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ'\n",
    "    for c in rus_alphavite:\n",
    "        s = s.replace(c, c.lower())\n",
    "    return s\n",
    "\n",
    "def check_eng(html_doc):\n",
    "    a = re.search('[а-яА-Я]', html_doc)\n",
    "    if a is None:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_lemms2(doc):\n",
    "    ans = []\n",
    "    for i in doc:\n",
    "        ans.append(i['lemma'])\n",
    "    return ans\n",
    "\n",
    "def select_spec(a):\n",
    "    if a[1] > 1:\n",
    "        return 'Data Science'\n",
    "    if a[2] > 1:\n",
    "        return 'Тестирование'\n",
    "    if a[0] > 2:\n",
    "        return 'DevOps'\n",
    "    return 'Разработка'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_dataset_201.csv', index_col=0)\n",
    "df_train_numpy = df_train.to_numpy()\n",
    "x = df_train_numpy[:, 0]\n",
    "y = df_train_numpy[:, 1].astype('int')\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', KNeighborsClassifier(n_neighbors = 6,weights = 'distance',algorithm = 'brute'))])\n",
    "model_knn = pipe.fit(x, y)\n",
    "\n",
    "def classify_section(title):\n",
    "    prediction = model_knn.predict([title])[0]\n",
    "    return prediction\n",
    "\n",
    "def extract_facts_from_vacancy_sections(sections, key_skills):\n",
    "    ans = [[], [], False, [0, 0, 0, 0]]\n",
    "    for i in sections:\n",
    "        target = classify_section(i[0])\n",
    "        doc = nlp(i[1])\n",
    "        res = get_full_result(doc)\n",
    "        if target == 1:\n",
    "            ans[0] += res[0]\n",
    "        if target == 2:\n",
    "            ans[1] += res[0]\n",
    "        if res[1]:\n",
    "            ans[2] = True\n",
    "        for j in range(4):\n",
    "            ans[3][j] += res[2][j]\n",
    "    \n",
    "    ans[0] = list(set(ans[0] + key_skills))\n",
    "    ans[1] = list(set(ans[1]))\n",
    "    ans[3] = select_spec(ans[3])\n",
    "    d = {'Обязательные компетенции': ans[0], 'Желательные компетенции': ans[1], 'Гибкий график работы': ans[2], 'Подобласть': ans[3]}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main html processing func, parse vacancy on simple sections\n",
    "def parse_html(html_doc):\n",
    "    if check_eng(html_doc):\n",
    "        print(\"This vacancy is English\")\n",
    "        return []\n",
    "    MAX_TITLE_LEN = 5\n",
    "    MAX_BODY_LEN = 10\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "    # process beginning part before <strong>\n",
    "    first_ul = soup.ul\n",
    "    try:\n",
    "        first_strong = first_ul.find_previous_sibling('p')\n",
    "    except Exception as e:\n",
    "        first_strong = soup.find_all('p')[-1]\n",
    "    if first_strong is None:\n",
    "        first_strong = first_ul.find_previous_sibling('strong')\n",
    "    informal_part = []\n",
    "    current_p = first_strong\n",
    "    result = []\n",
    "    try:\n",
    "        while current_p is not None and current_p.find_previous_sibling() is not None:\n",
    "            next_p = current_p.find_previous_sibling()\n",
    "\n",
    "            text = clear_string(next_p.text)\n",
    "            if len(text) == 0:\n",
    "                break\n",
    "            words = get_lemms2(process_doc(nlp(text)))\n",
    "            length = min(MAX_BODY_LEN, len(words))\n",
    "            body = ' '.join(words[: length])\n",
    "            informal_part.append(body)\n",
    "            current_p = next_p\n",
    "        informal_part.reverse()\n",
    "        for paragraph in informal_part:\n",
    "            result.append(['', paragraph])\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    # process <strong> and next <ul>\n",
    "    ptr = 1 if len(result) > 0 else 0\n",
    "    for ul in soup.find_all('ul'):\n",
    "        try:\n",
    "            text = clear_string(ul.find_previous_sibling().text)\n",
    "            words = get_lemms2(process_doc(nlp(text)))\n",
    "            length = min(MAX_TITLE_LEN, len(words))\n",
    "            title = words[:length]\n",
    "            title = ' '.join(title)\n",
    "            items = [] \n",
    "            for li in ul.find_all('li'):\n",
    "                s = clear_string(li.text)\n",
    "                \n",
    "                words = get_lemms2(process_doc(nlp(s)))\n",
    "                length = min(MAX_BODY_LEN, len(words))\n",
    "                body = words[:length]\n",
    "                body = ' '.join(body)\n",
    "                items.append(body)\n",
    "\n",
    "            for i in items:\n",
    "                result.append([title, i])\n",
    "            ptr += 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return result\n",
    "\n",
    "# main function to call\n",
    "def extract_facts_from_vacancy(vacancy):\n",
    "    html_doc = vacancy['description']\n",
    "    res1 = parse_html(html_doc)\n",
    "    key_skills = []\n",
    "    for i in vacancy['key_skills']:\n",
    "        key_skills.append(i['name'])\n",
    "    return extract_facts_from_vacancy_sections(res1, key_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Обязательные компетенции': ['Windows', 'CI', 'CD', 'Node.Js', 'RHEL', 'PHP', 'Redis', 'Linux', 'MySQL', 'Bash', 'Zabbix', 'CentOS', 'Администрирование серверов Linux', 'PostgreSQL', 'Nginx'], 'Желательные компетенции': ['Tomcat', 'Agile', 'ActiveMQ', 'Python', 'Apache'], 'Жесткий график работы': False, 'Подобласть': 'DevOps'}\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import json\n",
    "with codecs.open('vacancies10.json', 'r', encoding='utf8') as f:\n",
    "        vacancy = json.load(f)\n",
    "print(extract_facts_from_vacancy(vacancy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
